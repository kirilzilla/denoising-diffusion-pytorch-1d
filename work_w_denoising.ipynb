{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 6016, 128]) torch.Size([128])\n",
      "torch.Size([6016, 128, 202]) torch.Size([202])\n",
      "torch.Size([770048, 202])\n",
      "torch.Size([1, 770048, 202])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# create train_set , validation_set from merged npy array\n",
    "arr = torch.from_numpy(np.load('merged_array.npy'))\n",
    "# torch.Size([9494, 128, 128]) -> torch.Size([128, 128, 9494])\n",
    "print(arr.shape,arr[0][0].shape)\n",
    "arr = arr.permute(1, 2, 0)\n",
    "print(arr.shape,arr[0][0].shape)\n",
    "arr = arr.flatten(start_dim=0, end_dim=1)\n",
    "print(arr.shape)\n",
    "print(arr.unsqueeze(0).shape)\n",
    "#arr = arr.permute(1, 0)\n",
    "arr = arr.unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load 1d unet / diffusion model from gh repo, create example train_sequence and train. create diffusion bases sample version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44329fd287c04cf69670de40840e1d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd5dce8f476412ca1d92f471147669b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 202, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet1D, GaussianDiffusion1D, Trainer1D \n",
    "# import torch dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "model = Unet1D(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    channels = 202\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    model,\n",
    "    seq_length = 32,\n",
    "    timesteps = 100,\n",
    "    objective = 'pred_v'\n",
    ")\n",
    "\n",
    "training_seq = torch.rand(1, 202, 32) # features are normalized from 0 to 1 # b, ch, seq_length\n",
    "#training_seq = arr\n",
    "\n",
    "trainer = Trainer1D(\n",
    "    diffusion,\n",
    "    dataset = training_seq,\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 8e-5,\n",
    "    train_num_steps = 100,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# after a lot of training\n",
    "\n",
    "sampled_seq = diffusion.sample(batch_size = 4)\n",
    "sampled_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet1D(\n",
       "  (init_conv): Conv1d(202, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (time_mlp): Sequential(\n",
       "    (0): SinusoidalPosEmb()\n",
       "    (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (downs): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(64, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Conv1d(64, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(64, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(256, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(768, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv1d(768, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(512, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (1): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv1d(384, 256, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(256, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(192, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv1d(192, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "        (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (3): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (proj): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Residual(\n",
       "        (fn): PreNorm(\n",
       "          (fn): LinearAttention(\n",
       "            (to_qkv): Conv1d(64, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (mid_block1): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Identity()\n",
       "  )\n",
       "  (mid_attn): Residual(\n",
       "    (fn): PreNorm(\n",
       "      (fn): Attention(\n",
       "        (to_qkv): Conv1d(512, 384, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (to_out): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (mid_block2): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Identity()\n",
       "  )\n",
       "  (final_res_block): ResnetBlock(\n",
       "    (mlp): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "    (block1): Block(\n",
       "      (proj): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (res_conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (final_conv): Conv1d(64, 202, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 202, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4dda4422ef4ce68400a56a03b3b8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "seq length must be 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer1D(\n\u001b[1;32m      2\u001b[0m     diffusion,\n\u001b[1;32m      3\u001b[0m     dataset \u001b[39m=\u001b[39m arr[:,:,:\u001b[39m64\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     amp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,                       \u001b[39m# turn on mixed precision\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     13\u001b[0m \u001b[39m# after a lot of training\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sampled_seq \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39msample(batch_size \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/rsim/DiffusionCompression/denoising-diffusion-pytorch/denoising_diffusion_pytorch/denoising_diffusion_pytorch_1d.py:921\u001b[0m, in \u001b[0;36mTrainer1D.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    918\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    920\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m--> 921\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data)\n\u001b[1;32m    922\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_accumulate_every\n\u001b[1;32m    923\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/rsim/DiffusionCompression/denoising-diffusion-pytorch/denoising_diffusion_pytorch/denoising_diffusion_pytorch_1d.py:788\u001b[0m, in \u001b[0;36mGaussianDiffusion1D.forward\u001b[0;34m(self, img, *args, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    787\u001b[0m     b, c, n, device, seq_length, \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mimg\u001b[39m.\u001b[39mshape, img\u001b[39m.\u001b[39mdevice, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_length\n\u001b[0;32m--> 788\u001b[0m     \u001b[39massert\u001b[39;00m n \u001b[39m==\u001b[39m seq_length, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mseq length must be \u001b[39m\u001b[39m{\u001b[39;00mseq_length\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    789\u001b[0m     t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, (b,), device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    791\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(img)\n",
      "\u001b[0;31mAssertionError\u001b[0m: seq length must be 32"
     ]
    }
   ],
   "source": [
    "trainer = Trainer1D(\n",
    "    diffusion,\n",
    "    dataset = arr[:,:,:64],\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 8e-5,\n",
    "    train_num_steps = 100,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# after a lot of training\n",
    "\n",
    "sampled_seq = diffusion.sample(batch_size = 4)\n",
    "sampled_seq.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
